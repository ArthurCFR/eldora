# Guide de test - Agent V2

## ğŸš€ DÃ©marrage rapide

### 1. ArrÃªter V1 si actif

```bash
# Trouver le processus V1
ps aux | grep "python main.py"

# Tuer le processus
pkill -f "python main.py"
```

### 2. DÃ©marrer V2

```bash
cd agent
./start-v2.sh
```

Vous devriez voir :
```
ğŸš€ Starting Voyaltis Agent V2 (Ultra-Simplified)
================================================

Version: V2 - Simple linear question flow
Features: Minimal complexity, direct questions

Starting agent...
INFO:voyaltis-agent-v2:Worker started
```

### 3. VÃ©rifier les logs

Les logs de V2 incluent le tag `[V2]` :
```
ğŸš€ [V2] Starting simple agent for room: voyaltis-Thomas-1729850000
ğŸ¤ [V2] Simple agent started
âœ… [V2] Session running
```

## ğŸ§ª Test complet

### Terminal 1: Agent V2
```bash
cd agent
source venv/bin/activate
python mainV2.py start
```

### Terminal 2: Proxy Node.js
```bash
npm run proxy
```

### Terminal 3: App React Native
```bash
npm start
```

## âœ… Checklist de test

### Test 1: Conversation basique

**Attendu :**
1. âœ… Agent dit : "Salut [nom] ! Je vais te poser N questions rapides."
2. âœ… Agent pose question 1 (attention point 1)
3. âœ… User rÃ©pond
4. âœ… Agent pose question 2 (attention point 2)
5. âœ… User rÃ©pond
6. âœ… Agent dit : "Parfait ! Je vais prÃ©parer ton rapport."
7. âœ… Rapport gÃ©nÃ©rÃ© et affichÃ©

**Comment tester :**
- Ouvrir app sur tÃ©lÃ©phone/Ã©mulateur
- Appuyer sur bouton vocal
- Parler normalement
- VÃ©rifier que toutes les questions sont posÃ©es
- VÃ©rifier que le rapport est gÃ©nÃ©rÃ©

### Test 2: Logs V2

**Attendu :**
```
ğŸ“Š Will ask 2 questions
ğŸ’¬ assistant: Salut Thomas ! Je vais te poser 2 questions rapides.
ğŸ’¬ user: [transcription]
ğŸ“Š Questions: 1/2
ğŸ’¬ assistant: [question 2]
ğŸ“Š Questions: 2/2
ğŸ End detected - all questions asked + closing message
ğŸ“Š Generating report...
âœ… Report sent to client
```

**Comment tester :**
- Surveiller le terminal de l'agent V2
- VÃ©rifier la prÃ©sence du tag `[V2]`
- VÃ©rifier le compteur de questions

### Test 3: Configuration attention points

**Attendu :**
- Si 2 attention points configurÃ©s â†’ 2 questions
- Si 3 attention points configurÃ©s â†’ 3 questions
- Questions posÃ©es dans l'ordre des attention points

**Comment tester :**
1. Aller dans app admin
2. Ajouter un 3Ã¨me attention point : "Profil des visiteurs"
3. Sauvegarder
4. Lancer conversation
5. VÃ©rifier que 3 questions sont posÃ©es

### Test 4: GÃ©nÃ©ration du rapport

**Attendu :**
- Rapport avec tous les produits
- Sales correctement mappÃ©s (fuzzy matching)
- Customer feedback structurÃ© par sections
- Key insights gÃ©nÃ©rÃ©s

**Comment tester :**
1. ComplÃ©ter conversation
2. VÃ©rifier le rapport dans la modal
3. VÃ©rifier les ventes (nombres corrects)
4. VÃ©rifier customer_feedback (sections **BOLD**)
5. VÃ©rifier key_insights (2-4 insights)

### Test 5: Comparaison V1 vs V2

**Test A avec V1 :**
```bash
pkill -f mainV2
cd agent
python main.py start
```

**Test B avec V2 :**
```bash
pkill -f "python main.py"
cd agent
python mainV2.py start
```

**Comparer :**
| CritÃ¨re | V1 | V2 |
|---------|----|----|
| Temps de rÃ©ponse | ? | ? |
| NaturalitÃ© | ? | ? |
| Nombre de questions | ? | ? |
| QualitÃ© du rapport | ? | ? |

## ğŸ› Troubleshooting V2

### ProblÃ¨me : Agent ne dÃ©marre pas

**SymptÃ´mes :**
```
ModuleNotFoundError: No module named 'livekit'
```

**Solution :**
```bash
cd agent
source venv/bin/activate
pip install -r requirements.txt
```

### ProblÃ¨me : Pas de questions posÃ©es

**SymptÃ´mes :**
- Agent dit juste l'opening message puis se tait

**Debug :**
```bash
# VÃ©rifier les logs
tail -f agent/agent.log

# VÃ©rifier que metadata est reÃ§u
# Doit voir: "ğŸ‘¤ Loaded config: Thomas, 2 attention points"
```

**Solution :**
- VÃ©rifier que assistantConfig est bien passÃ© dans metadata
- VÃ©rifier que attention_points n'est pas vide

### ProblÃ¨me : Agent ne finit jamais

**SymptÃ´mes :**
- Toutes les questions posÃ©es mais pas de rapport

**Debug :**
```python
# Dans mainV2.py, ajouter plus de logs
logger.info(f"End check: {questions_asked} >= {max_questions}, keyword present: {'prÃ©parer ton rapport' in text_lower}")
```

**Solution :**
- VÃ©rifier que l'agent dit bien "prÃ©parer ton rapport"
- VÃ©rifier `should_end = True` dans les logs

### ProblÃ¨me : Rapport vide ou incorrect

**SymptÃ´mes :**
- Rapport gÃ©nÃ©rÃ© mais sales = tous Ã  0
- Customer feedback vide

**Debug :**
```python
# VÃ©rifier conversation_messages
logger.info(f"Conversation messages: {conversation_messages}")
```

**Solution :**
- VÃ©rifier que les messages sont bien stockÃ©s
- VÃ©rifier que Claude reÃ§oit la conversation complÃ¨te
- VÃ©rifier les prompts dans PromptBuilder

## ğŸ“Š MÃ©triques de performance V2

Ã€ collecter pendant les tests :

```
MÃ©trique                | V1      | V2      | Objectif
------------------------|---------|---------|----------
Temps total (3 Q)       | ?       | ?       | < 2 min
Latence par rÃ©ponse     | ?       | ?       | < 3s
Taux de complÃ©tion      | ?       | ?       | > 95%
QualitÃ© rapport (1-10)  | ?       | ?       | > 7/10
Bugs rencontrÃ©s         | ?       | ?       | 0
```

## ğŸ¯ Tests avancÃ©s

### Test stress : 10 conversations successives

```bash
# Script de test automatique (TODO)
for i in {1..10}; do
  echo "Test $i/10"
  # Lancer conversation
  # Attendre fin
  # VÃ©rifier rapport
done
```

### Test edge case : RÃ©ponses trÃ¨s courtes

**ScÃ©nario :**
```
Agent: Comment s'est passÃ©e ta journÃ©e ?
User: Bien.

Agent: Et les retours clients ?
User: Rien.
```

**Attendu :**
- Agent termine quand mÃªme
- Rapport gÃ©nÃ©rÃ© (peut-Ãªtre vide)
- Pas de crash

### Test edge case : RÃ©ponses trÃ¨s longues

**ScÃ©nario :**
```
Agent: Comment s'est passÃ©e ta journÃ©e ?
User: [Parle pendant 2 minutes sans s'arrÃªter]
```

**Attendu :**
- Whisper transcrit tout
- GPT gÃ©nÃ¨re question suivante
- Pas de timeout

### Test edge case : User interrompt agent

**ScÃ©nario :**
```
Agent: Et au niveau des ret--
User: J'ai aussi vendu des tablettes !
```

**Attendu :**
- Agent s'arrÃªte de parler
- Agent Ã©coute le user
- Agent reprend aprÃ¨s

## âœ… Validation finale

Avant de dÃ©ployer V2 en production :

- [ ] Test 1 (Conversation basique) : âœ… OK
- [ ] Test 2 (Logs V2) : âœ… OK
- [ ] Test 3 (Configuration) : âœ… OK
- [ ] Test 4 (Rapport) : âœ… OK
- [ ] Test 5 (Comparaison V1/V2) : âœ… OK
- [ ] Troubleshooting vÃ©rifiÃ©
- [ ] MÃ©triques collectÃ©es
- [ ] Tests edge cases passÃ©s
- [ ] Documentation Ã  jour
- [ ] README_V2.md relu

## ğŸ“ Notes de test

Date : ___________

Testeur : ___________

Version testÃ©e : V2.0.0

RÃ©sultats :
```
[Espace pour notes]
```

Bugs trouvÃ©s :
```
[Espace pour liste de bugs]
```

Recommandations :
```
[Espace pour recommandations]
```

---

**AprÃ¨s validation, n'oubliez pas de :**
1. Mettre Ã  jour TECHNICAL_DEEP_DIVE.md avec `/update-docs`
2. CrÃ©er un tag git : `git tag v2.0.0-simple`
3. Documenter les diffÃ©rences dans le changelog
